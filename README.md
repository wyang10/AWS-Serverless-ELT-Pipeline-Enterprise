# AWS Serverless ELT Pipeline (v2.0) â€” production-lite, enterprise features by toggles

> è½»é‡èµ·æ­¥ï¼Œä¼ä¸šåŒ–èƒ½åŠ›éšå¼€éšç”¨ï¼š**S3 â†’ Lambda â†’ SQS â†’ Lambda â†’ S3 (Parquet)**ï¼Œå¯é€‰ç¼–æ’ã€ç›®å½•ã€è´¨é‡é—¨ç¦ä¸å¯è§‚æµ‹æ€§ã€‚

This repo keeps the **minimal pipeline** as the default backbone, then layers optional â€œenterprise-ishâ€ modules:

- **Core pipeline**: S3 (bronze JSON/JSONL) â†’ Lambda ingest (object-level idempotency) â†’ SQS (+ DLQ) â†’ Lambda transform â†’ S3 (silver Parquet)
- **Orchestration (optional)**: Step Functions ops workflow for replay/backfill/quality polling (`make ops-start`)
- **Catalog / Query (optional)**: Glue Data Catalog + Crawler so Athena can query `silver/` as tables
- **Quality gate (optional)**: Glue Job + Step Functions (with optional auto-trigger via EventBridge)
- **Observability (optional)**: CloudWatch dashboard + alarms
- **Extensibility**: `make scaffold DATASET=<name>` generates dataset config/handler/DQ/sample skeleton

Roadmap: `ROADMAP.md`.

## ğŸ§© Architecture

```
S3 (bronze/*.jsonl)
  â””â”€(ObjectCreated)
     Lambda ingest (idempotent via DynamoDB)
        â””â”€ SQS (events) â”€â”€(event source mapping)â”€â”€> Lambda transform (Parquet)
              â””â”€ DLQ (optional)
                                â””â”€ S3 (silver/*.parquet)
```

## ğŸ” v1 vs v2.0 (what changed)

| Aspect | v1 (Minimal) | v2.0 (Enterprise-ish) |
|---|---|---|
| Pipeline | S3 â†’ Lambda â†’ SQS â†’ Lambda â†’ S3 | Same + optional orchestration |
| Idempotency | DynamoDB TTL (object-level) | Same + replay/backfill workflows |
| Failure handling | SQS + DLQ | Same + redrive helpers |
| Storage | JSONL â†’ Parquet | Same + optional compaction job |
| Queryability | S3 only | Glue Catalog/Crawler + Athena tables |
| Observability | Logs | Optional CloudWatch dashboard + alarms |
| Data quality | â€” | Optional quality gate (Glue Job + Step Functions) |
| Delivery | Local apply | GitHub Actions: CI + manual plan/apply (supports keys/OIDC) |

## ğŸ’¼ Resume / Interview snippets (pick one)

<details>
<summary>1-liner (LinkedIn / top of resume)</summary>

Built a production-lite serverless ELT framework on AWS (**S3 â†’ Lambda â†’ SQS â†’ Lambda â†’ S3 Parquet**) with optional orchestration, catalog, replay, and quality gating.

</details>

<details>
<summary>Resume bullets (3â€“5 bullets)</summary>

- Shipped a serverless ELT pipeline on AWS: S3 bronze JSONL â†’ Lambda ingest â†’ SQS (+ DLQ) â†’ Lambda transform â†’ S3 silver Parquet.
- Implemented object-level idempotency using DynamoDB conditional writes + TTL to prevent duplicate ingestion on retries/events.
- Added operational tooling: Step Functions replay/backfill workflow, SQS DLQ redrive, and one-command dataset scaffolding (`make scaffold DATASET=...`).
- Enabled â€œquery-readyâ€ silver layer via Glue Data Catalog + Crawler for Athena.
- Delivered infrastructure as code (Terraform modules) and CI automation (pytest + terraform fmt; manual Terraform plan/apply workflow).

</details>

<details>
<summary>Interview story (structured)</summary>

- Problem: Needed a simple but robust pipeline for JSONL â†’ Parquet without VPC/EC2, yet with real-world operability.
- Design: S3 event-driven ingest + SQS decoupling + batch transform to partitioned Parquet; add idempotency at the S3 object level.
- Reliability: Partial batch failure handling + DLQ + redrive; replay/backfill via Step Functions and S3-copy re-triggering.
- Operability: CloudWatch dashboard/alarms and a â€œquality gateâ€ workflow (Glue job orchestrated by Step Functions).

</details>

## Repo layout

```
repo-root/
â”œâ”€ README.md
â”œâ”€ Makefile
â”œâ”€ configs/                 # dataset configs (scaffolded)
â”œâ”€ dq/                      # per-dataset lightweight DQ rules (scaffolded)
â”œâ”€ data_samples/            # per-dataset JSONL samples (scaffolded)
â”œâ”€ templates/               # scaffolding templates
â”œâ”€ scripts/
â”‚  â”œâ”€ gen_fake_events.py
â”‚  â”œâ”€ replay_from_s3.py
â”‚  â”œâ”€ replay.sh             # wrapper: S3 copy replay
â”‚  â”œâ”€ redrive.sh            # wrapper: DLQ â†’ main queue (SQS native redrive)
â”‚  â””â”€ scaffold.sh           # generate new dataset skeleton
â”œâ”€ lambdas/
â”‚  â”œâ”€ ingest/
â”‚  â”‚  â”œâ”€ app.py
â”‚  â”‚  â”œâ”€ requirements.txt
â”‚  â”‚  â””â”€ tests/
â”‚  â”œâ”€ transform/
â”‚  â”‚  â”œâ”€ app.py
â”‚  â”‚  â”œâ”€ requirements.txt
â”‚  â”‚  â””â”€ tests/
â”‚  â””â”€ shared/
â”‚     â”œâ”€ __init__.py
â”‚     â”œâ”€ schemas.py
â”‚     â””â”€ utils.py
â””â”€ infra/
   â””â”€ terraform/
      â”œâ”€ backend/backend.hcl
      â”œâ”€ modules/
      â””â”€ envs/
         â””â”€ dev/
```

## Prereqs

- Python 3.11+
- Terraform 1.6+
- AWS credentials for a **dev** account (or a sandbox account)
- Optional: Docker (only if you prefer containerized builds)

## ä»é›¶å¼€å§‹ï¼ˆå®‰è£…/éƒ¨ç½²æŒ‡å¼•ï¼‰

ä¸‹é¢æ˜¯ä¸€å¥—â€œä»æ–° clone â†’ éƒ¨ç½² â†’ é€ æ•° â†’ éªŒè¯â€çš„å®Œæ•´å‘½ä»¤æ¸…å•ï¼ˆé»˜è®¤åŒºåŸŸ `us-east-2`ï¼‰ã€‚

### 0) Clone ä»£ç 

```bash
git clone https://github.com/wyang10/AWS-Serverless-ELT-Pipeline-Enterprise.git
cd AWS-Serverless-ELT-Pipeline-Enterprise

# å¯é€‰ï¼šåˆ‡åˆ° v2.0 tagï¼ˆæ›´é€‚åˆç®€å†/é‡Œç¨‹ç¢‘å±•ç¤ºï¼‰
git checkout v2.0
```

### 1) Python è™šæ‹Ÿç¯å¢ƒ + ä¾èµ–

```bash
python3 -m venv .venv
source .venv/bin/activate
python -m pip install -U pip
python -m pip install -r requirements-dev.txt
```

### 2) AWS å‡­è¯/åŒºåŸŸç¡®è®¤

```bash
# å¯é€‰ï¼šå¦‚æœä½ ç”¨çš„æ˜¯ profile
export AWS_PROFILE=<your-profile>

# å»ºè®®å›ºå®š regionï¼ˆTerraform/AWS CLI éƒ½ä¼šç”¨åˆ°ï¼‰
export AWS_REGION=us-east-2
export AWS_DEFAULT_REGION=us-east-2

aws sts get-caller-identity
```

å¦‚æœä½ å¸Œæœ›å›ºå®šä½¿ç”¨ IAM User profileï¼ˆä¾‹å¦‚ `audrey-tf`ï¼‰ä½†æœ¬æœºæ²¡æœ‰è¯¥ profileï¼š

```bash
make profile-audrey-tf
export AWS_PROFILE=audrey-tf
aws sts get-caller-identity
```

### 3) æœ¬åœ°å•æµ‹ï¼ˆå¯é€‰ä½†æ¨èï¼‰

```bash
source .venv/bin/activate
python -m pip install -U pip
python -m pip install -r requirements-dev.txt
make test

```

### 4) æ„å»º Lambda æ‰“åŒ…äº§ç‰©ï¼ˆbuild/*.zipï¼‰

```bash
make build
```

### 5) Terraform åˆå§‹åŒ– + éƒ¨ç½²

```bash
make tf-init
TF_AUTO_APPROVE=1 make tf-apply
```

è¯´æ˜ï¼š`infra/terraform/envs/dev/dev.tfvars` é‡Œé€šè¿‡å¼€å…³æ§åˆ¶å¯é€‰æ¨¡å—ï¼ˆops/glue/ge/observabilityï¼‰ã€‚æœ¬ repo é»˜è®¤ `observability_enabled=true`ï¼›å¦‚æœä½ çš„è´¦å·ç¼ºå°‘ CloudWatch `PutMetricAlarm/PutDashboard` æƒé™ï¼Œå¯å…ˆè®¾ä¸º `false` å† applyã€‚

å¦‚æœä½ é‡åˆ° `sqs:ListQueueTags` çš„ 403ï¼ˆæœ‰äº›è´¦å·ç¦æ­¢è¯» tagsï¼‰ï¼Œç”¨ä¸‹é¢â€œæ—  tag APIâ€çš„æ–¹å¼ç»•è¿‡ï¼š

```bash
# 1) å…ˆåœ¨ AWS é‡Œåˆ›å»ºé˜Ÿåˆ— + DLQï¼Œå¹¶æŠŠ URL/ARN å†™åˆ° auto tfvarsï¼ˆTerraform ä¼šè‡ªåŠ¨è¯»å–ï¼‰
python3 scripts/create_sqs_queue.py \
  --name serverless-elt-<suffix>-events \
  --with-dlq \
  --region us-east-2 \
  --out infra/terraform/envs/dev/queue.auto.tfvars.json

# 2) å¦‚æœä¹‹å‰ Terraform å·²ç»æŠŠ queue èµ„æºå†™å…¥ stateï¼Œéœ€è¦ç§»é™¤å¯¹åº”æ¡ç›®ï¼ˆé¿å… refresh å†è§¦å‘ ListQueueTagsï¼‰
terraform -chdir=infra/terraform/envs/dev state list | rg '^module\\.queue' || true
terraform -chdir=infra/terraform/envs/dev state rm 'module.queue[0].aws_sqs_queue.dlq[0]' || true

# 3) å†æ¬¡ apply
TF_AUTO_APPROVE=1 make tf-apply
```

### 6) é€ æ•°å¹¶ä¸Šä¼ åˆ° Bronzeï¼ˆè§¦å‘æ•´æ¡ç®¡é“ï¼‰

```bash
BRONZE=$(terraform -chdir=infra/terraform/envs/dev output -raw bronze_bucket)

python3 scripts/gen_fake_events.py --type shipments --count 50 --format jsonl --out /tmp/shipments.jsonl
aws s3 cp /tmp/shipments.jsonl \
  "s3://$BRONZE/bronze/shipments/manual/shipments-$(date -u +%Y%m%dT%H%M%SZ).jsonl" \
  --region us-east-2
```

### 7) éªŒè¯ Silver äº§å‡º Parquet

```bash
SILVER=$(terraform -chdir=infra/terraform/envs/dev output -raw silver_bucket)
aws s3 ls "s3://$SILVER/silver/shipments/" --recursive --region us-east-2 | tail
```

### 8) v2 â€œä¼ä¸šåŒ–â€èƒ½åŠ›ï¼ˆæŒ‰éœ€ï¼‰

```bash
# Ops Step Functionsï¼ˆå›çŒ/å·¡æ£€ç¼–æ’ï¼‰
make ops-start
make ops-status
make ops-history

# Glue Crawlerï¼ˆè®© Athena æœ‰è¡¨ï¼‰
make glue-crawler-start
make glue-crawler-status

# Glue Compaction Jobï¼ˆå°æ–‡ä»¶å‹ç¼©/æŒ‰å¤©é‡è·‘ï¼Œè¾“å‡ºåˆ°å®‰å…¨å‰ç¼€ï¼‰
make glue-job-start GLUE_RECORD_TYPE=shipments GLUE_DT=2025-12-31 GLUE_OUTPUT_PREFIX=silver_compacted
make glue-job-status

# GE Quality Gateï¼ˆGlue Job + Step Functions é—¸é—¨ï¼‰
make ge-start GE_RECORD_TYPE=shipments GE_DT=2025-12-31 GE_RESULT_PREFIX=ge/results
make ge-status
make ge-history
```

## GitHub Actionsï¼ˆæ›´åƒä¼ä¸šäº¤ä»˜ï¼‰

æœ¬ä»“åº“åŒ…å«ï¼š

- `.github/workflows/ci.yml`ï¼š`pytest` + `terraform fmt -check`
- `.github/workflows/terraform-manual.yml`ï¼šæ‰‹åŠ¨è§¦å‘çš„ `plan/apply/destroy`ï¼ˆä¼ä¸šç‰ˆå·¥ä½œæµï¼‰

### terraform-manualï¼šå…ˆé… Secrets å†è¿è¡Œ

è·¯å¾„ï¼šGitHub Repo â†’ Settings â†’ Secrets and variables â†’ Actions â†’ New repository secret

`terraform-manual` éœ€è¦åœ¨ Repo Secrets é…ç½® AWS å‡­è¯ï¼š

- è§¦å‘æ—¶é€‰æ‹© `auth=keys`ï¼šéœ€è¦ `AWS_ACCESS_KEY_ID` / `AWS_SECRET_ACCESS_KEY`ï¼ˆå¯é€‰ `AWS_SESSION_TOKEN`ï¼‰
- è§¦å‘æ—¶é€‰æ‹© `auth=oidc`ï¼šéœ€è¦ `AWS_ROLE_TO_ASSUME`ï¼ˆOIDC AssumeRoleï¼‰
- `TF_BACKEND_HCL`ï¼šè¿œç«¯ backend é…ç½®ï¼ˆä¸æä¾›æ—¶åªå…è®¸ `plan`ï¼›ä¼šé˜»æ­¢ `apply/destroy` ä»¥é¿å… runner æœ¬åœ° stateï¼‰

## Dataset Scaffoldï¼ˆå¿«é€Ÿæ‰©å±•æ–°æ•°æ®é›†ï¼‰

ä¸€è¡Œç”Ÿæˆ 4 ä¸ªæ–‡ä»¶ï¼ˆconfig/handler/dq/sampleï¼‰ï¼š

```bash
make scaffold DATASET=ups_shipping
ls -la configs/ups_shipping.yaml lambdas/transform/ups_shipping/handler.py dq/ups_shipping/rules.yaml data_samples/ups_shipping/sample.jsonl
```

ä¸‹ä¸€æ­¥ï¼š

- ç¼–è¾‘ `configs/<dataset>.yaml`ï¼ˆprefix/idempotency_key/output_columnsï¼‰
- å®ç° `lambdas/transform/<dataset>/handler.py` çš„å­—æ®µæ˜ å°„
- ï¼ˆå¯é€‰ï¼‰åœ¨ `dq/<dataset>/rules.yaml` è¡¥å……è½»é‡è§„åˆ™æˆ–æ˜ å°„åˆ° GE

## E2E éªŒæ”¶æ¸…å•ï¼ˆè§æˆªå›¾ï¼‰

ç›®æ ‡ï¼šæŒ‰é¡ºåºè¿‡ä¸€éå°±èƒ½ç•™å­˜è¯æ®ã€‚æ¯ä¸€æ­¥éƒ½æœ‰ã€Œè¦ç‚¹ â†’ å‘½ä»¤/æ§åˆ¶å°è·¯å¾„ â†’ é€šè¿‡æ ‡å‡†ï¼ˆæˆªå›¾ç‚¹ï¼‰ã€ã€‚

å»ºè®®å…ˆè·‘ä¸€é”®ç‰ˆï¼ˆä¼šä¾æ¬¡æ‰§è¡Œå¤šæ­¥ CLI éªŒè¯ + é€ æ•° + ç­‰å¾… Silverï¼‰ï¼š

- `make verify-e2e`

![](<demo/0-1.png>)
![](<demo/0-2.png>)

å¦‚æœä½ å¸Œæœ›å›ºå®šç”¨ IAM User profileï¼ˆä¾‹å¦‚ `audrey-tf`ï¼‰ï¼Œä½†æœ¬æœºæ²¡æœ‰è¯¥ profileï¼š

- `make profile-audrey-tf`
- ç„¶åï¼š`export AWS_PROFILE=audrey-tf`

### 0) æˆ‘æ˜¯è°ï¼ˆProfile/Regionï¼‰

è¦ç‚¹ï¼šç¡®è®¤å½“å‰å‘½ä»¤è¡Œä½¿ç”¨çš„ AWS èº«ä»½ä¸åŒºåŸŸã€‚

- å‘½ä»¤ï¼š`make verify-whoami`
- é€šè¿‡ï¼š`Account=818466672474`ï¼Œä¸” Arn å¯¹åº”ä½ é¢„æœŸçš„èº«ä»½ï¼ˆå¯ä¸º `user/audrey-tf` æˆ– Toolkit ç™»å½•çš„ sessionï¼‰ã€‚
- æˆªå›¾ï¼šç»ˆç«¯è¾“å‡ºæ•´å—ï¼ˆå« `AWS_PROFILE` / `AWS_REGION`ï¼‰ã€‚
![](<demo/0-æˆ‘æ˜¯è°(Profile:Region+Pythonç¯å¢ƒ).png>)

### 1) Terraform è¾“å‡ºå°±ç»ª

è¦ç‚¹ï¼šç¡®ä¿ TF å·²éƒ¨ç½²å¹¶ä¸”èƒ½è¾“å‡ºå…³é”®èµ„æºå/ARNã€‚

- å‘½ä»¤ï¼š`make verify-tf-outputs`
- é€šè¿‡ï¼šè¾“å‡ºé‡Œè‡³å°‘åŒ…å« `bronze_bucket` / `silver_bucket` / `queue_url` / `ingest_lambda` / `transform_lambda`ã€‚
- æˆªå›¾ï¼šç»ˆç«¯ outputã€‚
![](<demo/1-Terraformç”¨å¯¹äº†èº«ä»½+Region.png>)
![](<demo/1-Terraformç”¨å¯¹äº†èº«ä»½+Region-2.png>)


### 2) S3 äº‹ä»¶è§¦å‘å·²å°±ç»ªï¼ˆbronze â†’ ingestï¼‰

è¦ç‚¹ï¼šS3 Event notification æŒ‡å‘ ingest Lambdaï¼Œå¹¶ä¸” prefix ä¸º `bronze/`ã€‚

- å‘½ä»¤ï¼š`make verify-s3-notifications`
- æ§åˆ¶å°ï¼šS3 â†’ `<bronze_bucket>` â†’ Properties â†’ Event notifications
- é€šè¿‡ï¼šäº‹ä»¶å­˜åœ¨ä¸”ç›®æ ‡ ARN ä¸º ingestã€‚
- æˆªå›¾ï¼šæ§åˆ¶å° Event notification å¡ç‰‡ï¼ˆæˆ–ç»ˆç«¯ table è¾“å‡ºï¼‰ã€‚
![](<demo/2-S3äº‹ä»¶è§¦å‘å·²å°±ç»ª(bronzeâ†’LambdaIngest).png>)

### 3) Lambda å­˜åœ¨ä¸”å¯è¯»ï¼ˆingest / transformï¼‰

- å‘½ä»¤ï¼š`make verify-lambdas`
- é€šè¿‡ï¼šè¾“å‡º `OK ingest=...`ã€`OK transform=...`
- æˆªå›¾ï¼šç»ˆç«¯è¾“å‡ºï¼›æ§åˆ¶å° Lambda â†’ Monitor â†’ Logs
![](<demo/3-Lambdaæ­£å¸¸(ingest-transform).png>)

### 4) å¹‚ç­‰è¡¨ï¼ˆDynamoDBï¼‰ä¸ TTLï¼ˆå¯¹è±¡çº§åˆ«ï¼‰

è¦ç‚¹ï¼šæœ¬é¡¹ç›®å¹‚ç­‰ç²’åº¦æ˜¯ **S3 å¯¹è±¡çº§åˆ«**ï¼š`s3://bucket/key#etag`ï¼Œä¸æ˜¯ record/event_id çº§åˆ«ã€‚

- å‘½ä»¤ï¼š`make verify-ddb`
- é€šè¿‡ï¼šTTL `ENABLED`ï¼›scan èƒ½çœ‹åˆ° `pk/status` ç­‰å­—æ®µã€‚
- æˆªå›¾ï¼šç»ˆç«¯ TTL è¾“å‡º + scan è¾“å‡ºï¼ˆå‰å‡ æ¡ï¼‰ã€‚
![](<demo/4-å¹‚ç­‰è¡¨-DynamoDB-TTL.png>)

### 5) SQS / DLQ å¥åº·

- å‘½ä»¤ï¼š`make verify-sqs`
- é€šè¿‡ï¼šä¸»é˜Ÿåˆ—æ¶ˆæ¯æ•°æ¥è¿‘ 0ï¼›DLQ ä¸º 0ã€‚ï¼ˆæ¶ˆæ¯â€œæœ€è€å¹´é¾„â€å±äº CloudWatch æŒ‡æ ‡ï¼Œä¸æ˜¯ SQS attributeï¼‰
- æˆªå›¾ï¼šç»ˆç«¯ `get-queue-attributes` è¾“å‡ºï¼› SQS æ§åˆ¶å° Monitoring å›¾è¡¨ã€‚
![](<demo/5-SQS-DLQå¥åº·.png>)

### 6) é€ æ•°è§¦å‘ E2Eï¼ˆS3 â†’ ingest â†’ SQS â†’ transform â†’ Silverï¼‰

è¦ç‚¹ï¼šä¸Šä¼ ä¸€ä»½ Bronze JSONL è§¦å‘æ•´æ¡é“¾è·¯ã€‚

- å‘½ä»¤ï¼š`make verify-seed`ï¼ˆä¼šæŠŠæœ¬æ¬¡ä¸Šä¼ å†™å…¥ `$(E2E_LAST_SEED_FILE)`ï¼‰
- é€šè¿‡ï¼šä¸Šä¼ æˆåŠŸï¼ˆç»ˆç«¯ä¼šæ‰“å° `s3://<bronze>/<key>`ï¼‰
- æˆªå›¾ï¼šç»ˆç«¯è¾“å‡º + S3 æ§åˆ¶å°é‡Œè¯¥å¯¹è±¡ Key
![](<demo/é€ æ•°è§¦å‘E2E(S3-ingest-SQS-transform-Silver).png>)

### 7) Silver äº§å‡º Parquetï¼ˆç­‰å¾…çª—å£ï¼‰

- å‘½ä»¤ï¼š`make verify-silver`
- é€šè¿‡ï¼šæœ€è¿‘ `$(VERIFY_WINDOW_MINUTES)` åˆ†é’Ÿå†…èƒ½è§‚æµ‹åˆ° `silver/shipments/` ä¸‹æ–°å¢ parquetã€‚
- æˆªå›¾ï¼šç»ˆç«¯ OK è¾“å‡ºï¼›æˆ– S3 æ§åˆ¶å°æ˜¾ç¤º parquet æ–‡ä»¶åˆ—è¡¨ã€‚
![](<demo/6-é€ æ•°è§¦å‘æ•´æ¡é“¾è·¯(S3â†’ingestâ†’SQSâ†’transformâ†’Silver).png>)

### 8) å¹‚ç­‰éªŒæ”¶ï¼ˆåŒä¸€å¯¹è±¡é‡å¤è§¦å‘ä¼šè¢«è·³è¿‡ï¼‰

è¦ç‚¹ï¼šä¸ºäº†é¿å… S3 è‡ªåŠ¨è§¦å‘å¹²æ‰°ï¼Œè¿™ä¸€æ­¥ä¼šæŠŠå¯¹è±¡ä¸Šä¼ åˆ° `$(E2E_IDEMPOTENCY_PREFIX)/...`ï¼ˆä¸åŒ¹é… `bronze/` é€šçŸ¥ï¼‰ï¼Œç„¶åæ‰‹åŠ¨ invoke ingest ä¸¤æ¬¡ï¼Œç¬¬äºŒæ¬¡åº” `skipped>=1`ã€‚

- å‘½ä»¤ï¼š`make verify-idempotency`
- é€šè¿‡ï¼šç¬¬äºŒæ¬¡ invoke è¾“å‡ºé‡Œ `skipped>=1`ï¼Œå¹¶æ‰“å° `OK: second invoke skipped`ã€‚
- æˆªå›¾ï¼šç»ˆç«¯ first/second ä¸¤æ®µè¾“å‡ºã€‚
![](<demo/7-å¹‚ç­‰éªŒæ”¶-profile-key-lambda-invoke.png>)

### 9) Glue / Athenaï¼ˆå¯é€‰ï¼‰

å‰æï¼š`glue_enabled=true`ã€‚

- å‘½ä»¤ï¼š`make verify-glue`
- é€šè¿‡ï¼šcrawler `LastCrawl=SUCCEEDED`ï¼›èƒ½çœ‹åˆ° database åç§°ã€‚
- æˆªå›¾ï¼šGlue æ§åˆ¶å°è¡¨ç»“æ„é¡µ + Athena æŸ¥è¯¢ç»“æœï¼ˆè§ä¸‹æ–¹ â€œAthena quick queriesâ€ï¼‰ã€‚
![](<demo/8-GlueCatalog-Athenaå¯æŸ¥è¯¢.png>)
![](<demo/Athena.png>)

### 10) GE è´¨é‡é—¨ç¦ï¼ˆå¯é€‰ï¼‰

å‰æï¼š`ge_enabled=true` ä¸” `ge_workflow_enabled=true`ã€‚

- å‘½ä»¤ï¼š`make verify-ge`ï¼ˆåˆ—å‡ºæœ€è¿‘æ‰§è¡Œï¼›æ‰‹åŠ¨è§¦å‘ç”¨ `make ge-start`ï¼‰
- é€šè¿‡ï¼šèƒ½çœ‹åˆ°æœ€è¿‘çš„ executionsï¼›æˆ–è‡³å°‘ state machine å­˜åœ¨ã€‚
- æˆªå›¾ï¼šStep Functions execution è¯¦æƒ…ï¼ˆGlue task ç»¿å‹¾ï¼‰æˆ–ç»ˆç«¯ list-executions è¾“å‡ºã€‚
![](<demo/9-è´¨é‡é—¨ç¦-StepFunctions-GlueGEJob.png>)
![](<demo/step-function-1.png>)
![](<demo/step-function-2.png>)

### 11) CloudWatch Dashboardï¼ˆå¯é€‰ï¼‰

å‰æï¼š`observability_enabled=true` ä¸”è´¦å·å…è®¸ `cloudwatch:PutDashboard`ã€‚

- å‘½ä»¤ï¼š`make verify-observability`
- é€šè¿‡ï¼šè¾“å‡º `OK dashboard=...`
- æˆªå›¾ï¼šCloudWatch Dashboard é¢„è§ˆé¡µã€‚

## Quickstart (dev)

1) Build Lambda zips:

- `python -m pip install -r requirements-dev.txt`
- `make build`

2) Deploy infra:

- `make tf-init`
- `TF_AUTO_APPROVE=1 make tf-apply`

3) Upload raw events into Bronze:

- `python scripts/gen_fake_events.py --type shipments --count 50 --format jsonl --out /tmp/shipments.jsonl`
- `python scripts/gen_fake_events.py --type shipments --count 50 --format json --out /tmp/shipments.json`
- `aws s3 cp /tmp/shipments.jsonl s3://<bronze_bucket>/bronze/shipments/dt=2025-01-01/shipments.jsonl`

4) Verify outputs:

- Check CloudWatch logs for both Lambdas
- Look for Parquet objects under `s3://<silver_bucket>/silver/<type>/dt=YYYY-MM-DD/`

## Replay / backfill

Two options depending on your IAM permissions:

- **Replay via S3 copy (recommended)**: copies objects to a new key under `bronze/` so the normal S3 â†’ ingest â†’ SQS path runs (does not require your user to have `sqs:SendMessage`).
  - `python scripts/replay_via_s3_copy.py --bucket <bronze_bucket> --prefix bronze/shipments/ --dest-prefix bronze/replay/2026-01-01T00-00-00Z --start 2026-01-01T00:00:00Z --end 2026-01-02T00:00:00Z`
- **Replay directly to SQS**: reads Bronze objects and publishes events straight into SQS (requires `sqs:SendMessage` on the queue).
  - `python scripts/replay_from_s3.py --bucket <bronze_bucket> --prefix bronze/shipments/ --queue-url <queue_url> --start 2026-01-01T00:00:00Z --end 2026-01-02T00:00:00Z`

## Ops workflow (EventBridge + Step Functions)

The v2 track adds a minimal â€œopsâ€ workflow that can be run manually or on a schedule:

- **Replay/backfill** by copying S3 objects to a new key under `bronze/` (triggers the normal ingest path)
- **Inspect** by checking Silver contains recent Parquet outputs (poll + retry)

Terraform wiring lives in:

- `infra/terraform/modules/workflow_ops/main.tf`
- `infra/terraform/envs/dev/main.tf`

To enable in dev:

- Build artifacts (adds `build/ops_replay.zip` + `build/ops_quality.zip`): `make build`
- Set in `infra/terraform/envs/dev/dev.tfvars`:
  - `ops_enabled = true`
  - `ops_workflow_id = "ops-replay-and-quality-gate"`
  - Optional schedule: `ops_schedule_enabled = true` and `ops_schedule_expression = "rate(1 day)"`
- Deploy: `make tf-init` then `TF_AUTO_APPROVE=1 make tf-apply`

To start it manually:

- One-liners:
  - Start: `make ops-start`
  - Status: `make ops-status`
  - History: `make ops-history`
- Or raw CLI (if you prefer):
  - `aws stepfunctions start-execution --region us-east-2 --state-machine-arn $(terraform -chdir=infra/terraform/envs/dev output -raw ops_state_machine_arn) --input '{
    "bronze_bucket":"<bronze_bucket>",
    "src_prefix":"bronze/shipments/",
    "dest_prefix_base":"bronze/replay/manual",
    "window_hours":24,
    "silver_bucket":"<silver_bucket>",
    "silver_prefix":"silver",
    "record_type":"shipments",
    "min_parquet_objects":1,
    "poll_interval_seconds":30,
    "max_attempts":20
  }'`

## Notes

- **Idempotency scope**: ingest is idempotent at *S3 object* granularity (`bucket/key#etag`).
- **Visibility**: both Lambdas log structured JSON lines; transform uses SQS partial batch response.
- **Cost**: this uses only S3/SQS/Lambda/DynamoDB/CloudWatch.

## Glue Data Catalog (Catalog + Crawler)

Enable the crawler so Athena (or a future warehouse) can query Silver Parquet as tables.

- Set in `infra/terraform/envs/dev/dev.tfvars`:
  - `glue_enabled = true`
  - Optional: `glue_silver_prefix = "silver/"` (default) and `glue_table_prefix = "silver_"`
- Deploy: `TF_AUTO_APPROVE=1 make tf-apply`
- Run the crawler:
  - Start: `make glue-crawler-start`
  - Status: `make glue-crawler-status`

After the crawler finishes, you should see a Glue database + tables; then you can query via Athena.

## Glue Job (Compaction / Recompute)

Optional next step: move â€œrecompute / small-file compaction / per-day rerunâ€ to a Glue job.

- Enable the job in `infra/terraform/envs/dev/dev.tfvars`:
  - `glue_job_enabled = true`
  - Optional: `glue_job_name = "<name>"` and `glue_job_script_key = "glue/scripts/compact_silver.py"`
- Deploy: `TF_AUTO_APPROVE=1 make tf-apply`
- Run a compaction for a single partition:
  - Start: `make glue-job-start GLUE_RECORD_TYPE=shipments GLUE_DT=2025-12-31 GLUE_OUTPUT_PREFIX=silver_compacted`
  - Status: `make glue-job-status`

## Great Expectations (Quality Gate)

Run Great Expectations inside a Glue job, orchestrated by Step Functions as a â€œquality gateâ€.

- Enable in `infra/terraform/envs/dev/dev.tfvars`:
  - `ge_enabled = true` (creates the Glue GE job)
  - `ge_workflow_enabled = true` (creates the Step Functions gate)
  - Optional auto-trigger from transform:
    - `ge_emit_events_from_transform = true`
    - `ge_eventbridge_enabled = true`
  - Optional failure handling:
    - `ge_notification_topic_arn = "<sns_topic_arn>"` (or reuse `alarm_notification_topic_arn`)
    - `ge_quarantine_enabled = true` (writes a marker JSON to `ge_quarantine_prefix`)
- Deploy: `TF_AUTO_APPROVE=1 make tf-apply`
- Manual run:
  - `make ge-start GE_RECORD_TYPE=shipments GE_DT=2025-12-31 GE_RESULT_PREFIX=ge/results`
  - `make ge-status`
  - `make ge-history`

### Auto-trigger (optional)

Two toggles control whether the GE gate runs automatically after `transform` writes a partition:

- Keep **manual** (recommended for dev / controlled runs):
  - `ge_emit_events_from_transform = false`
  - `ge_eventbridge_enabled = false`
- Enable **auto-trigger** (recommended for â€œprod-likeâ€ behavior):
  - `ge_emit_events_from_transform = true` (transform emits an EventBridge event per written partition)
  - `ge_eventbridge_enabled = true` (EventBridge rule starts the GE state machine)

When auto-trigger is on, every successful transform write may trigger a validation run; keep it off if you are iterating quickly or donâ€™t want extra runs/cost.

## Athena quick queries

Get the Glue database name from Terraform:

- `terraform -chdir=infra/terraform/envs/dev output -raw glue_database_name`

Then run queries (replace `<db>` with that value):

```sql
-- 1) Latest shipments
SELECT dt, shipment_id, origin, destination, carrier, weight_kg, event_time
FROM "<db>".silver
WHERE record_type = 'shipments'
ORDER BY dt DESC, event_time DESC
LIMIT 20;

-- 2) Row counts per partition/day
SELECT dt, COUNT(*) AS rows
FROM "<db>".silver
WHERE record_type = 'shipments'
GROUP BY dt
ORDER BY dt DESC;

-- 3) Simple sanity checks (range / uniqueness signal)
SELECT
  dt,
  MIN(weight_kg) AS min_weight_kg,
  MAX(weight_kg) AS max_weight_kg,
  APPROX_DISTINCT(shipment_id) AS approx_unique_shipments
FROM "<db>".silver
WHERE record_type = 'shipments'
GROUP BY dt
ORDER BY dt DESC;
```

## Cleanup (cost control)

Terraform destroy will remove most resources, but **S3 buckets must be empty** first.

- Destroy:
  - `TF_AUTO_APPROVE=1 make tf-destroy`
- If destroy fails because buckets are not empty:
  - `BRONZE=$(terraform -chdir=infra/terraform/envs/dev output -raw bronze_bucket)`
  - `SILVER=$(terraform -chdir=infra/terraform/envs/dev output -raw silver_bucket)`
  - `aws s3 rm "s3://$BRONZE" --recursive --region us-east-2`
  - `aws s3 rm "s3://$SILVER" --recursive --region us-east-2`
  - Re-run: `TF_AUTO_APPROVE=1 make tf-destroy`

Notes:

- If you enabled Glue resources, Terraform will attempt to delete the Glue database/crawler/job; if anything is left behind, remove it in the Glue console.
- If you used an externally-managed SQS queue via `infra/terraform/envs/dev/*.auto.tfvars.json`, Terraform wonâ€™t delete that queue; delete it separately if needed.

### IAM gotchas

Some orgs allow creating IAM roles but **disallow tagging IAM/SQS** (missing `iam:TagRole` / `sqs:TagQueue`), which can show up as `AccessDenied` on `CreateRole`/`CreateQueue` when tags are included.
This repo disables tags for IAM roles and SQS queues by default in `infra/terraform/envs/dev/main.tf:1`.

If you still hit IAM/SQS permissions:

- **IAM role name restrictions**: set `iam_name_prefix` in `infra/terraform/envs/dev/dev.tfvars:1` to a permitted prefix.
- **SQS tag read restrictions** (e.g., missing `sqs:ListQueueTags`): pre-create the queue and feed Terraform:
  - Easiest fix is to allow `sqs:ListQueueTags` (read-only) on your IAM user/role.
  - `python scripts/create_sqs_queue.py --name <project>-<suffix>-events --with-dlq --region us-east-2 --out infra/terraform/envs/dev/queue.auto.tfvars.json`
  - Terraform will auto-load `*.auto.tfvars.json` and skip managing SQS when `existing_queue_url`/`existing_queue_arn` are provided.
- **Attach a DLQ to an existing queue**:
  - `python scripts/ensure_dlq_for_queue.py --queue-url <queue_url> --dlq-name <queue_name>-dlq --region us-east-2`
  - Then add `existing_dlq_url` / `existing_dlq_arn` into `infra/terraform/envs/dev/queue.auto.tfvars.json` (optional; used for outputs/observability).

### Transform dependency

The transform function writes Parquet via the AWS SDK for pandas layer (includes `pyarrow`) configured in `infra/terraform/envs/dev/dev.tfvars:4`.

### Observability (alarms/dashboard)

Terraform can create CloudWatch alarms + a dashboard via `infra/terraform/modules/observability/main.tf:1`, but many restricted dev accounts block:

- `cloudwatch:PutMetricAlarm`
- `cloudwatch:PutDashboard`

In that case, set `observability_enabled = false` in `infra/terraform/envs/dev/dev.tfvars:5` and re-apply (this repo defaults it to `true`).

## License

MIT â€” see `LICENSE`.
